{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于梯度的优化"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">本文内容：\n",
    "1.什么是梯度下降方法？\n",
    "2.反向传播算法（BP算法）\n",
    "\n",
    "## 1.为什么要梯度优化\n",
    "上一节介绍过，我们的第一个神经网络示例中，每个神经层都用下述方法对输入数据进行 变换。\n",
    "```\n",
    "output = relu(dot(W, input) + b)\n",
    "```\n",
    "在这个表达式中，W 和 b 都是张量，均为该层的属性。它们被称为该层的`权重(weight)`或 `可训练参数(trainable parameter)`，分别对应 kernel 和 bias 属性。\n",
    "\n",
    "这些权重包含网络从观察 训练数据中学到的信息。\n",
    "\n",
    "一开始，这些权重矩阵取较小的随机值，这一步叫作`随机初始化(random initialization)`。 \n",
    "\n",
    "当然，W 和 b 都是随机的，relu(dot(W, input) + b) 肯定不会得到任何有用的表示。\n",
    "\n",
    "虽然得到的表示是没有意义的，但这是一个起点。下一步则是根据反馈信号逐渐调节这些权重。这个逐渐调节的过程叫作训练，也就是机器学习中的学习。\n",
    "\n",
    "上述过程发生在一个训练循环(training loop)内，其具体过程如下。必要时一直重复这些步骤。\n",
    "\n",
    ">(1) 抽取训练样本x和对应目标y组成的数据批量。\n",
    "(2) 在 x 上运行网络[这一步叫作前向传播(forward pass)]，得到预测值 y_pred。\n",
    "(3) 计算网络在这批数据上的损失，用于衡量y_pred和y之间的距离。\n",
    "(4) 更新网络的所有权重，使网络在这批数据上的损失略微下降。\n",
    "\n",
    "最终得到的网络在训练数据上的损失非常小，即预测值 y_pred 和预期目标 y 之间的距离非常小。网络“学会”将输入映射到正确的目标。乍一看可能像魔法一样，但如果你将其简化为基本步骤，那么会变得非常简单。\n",
    "\n",
    "第一步看起来非常简单，只是输入 / 输出(I/O)的代码。第二步和第三步仅仅是一些张量运算的应用，所以你完全可以利用上一节学到的知识来实现这两步。\n",
    "*难点在于第四步:更新网络的权重，也就是著名的BP误差逆传播算法。*\n",
    "\n",
    "考虑网络中某个权重系数，你怎么知道这个系数应该增大还是减小，以及变化多少?\n",
    "就像我们开车，上一秒车头往左偏了30度，接下来我们要计算下一秒方向盘向右打多少度才能板正车头？\n",
    "\n",
    "一种简单的解决方案是，保持网络中其他权重不变，只考虑某个标量系数，让其尝试不同 的取值。假设这个系数的初始值为 0.3。对一批数据做完前向传播后，网络在这批数据上的损失 是 0.5。如果你将这个系数的值改为 0.35 并重新运行前向传播，损失会增大到 0.6。但如果你将 这个系数减小到 0.25，损失会减小到 0.4。在这个例子中，将这个系数减小 0.05 似乎有助于使\n",
    "损失最小化。对于网络中的所有系数都要重复这一过程。\n",
    "\n",
    "但这种方法是非常低效的，因为对每个系数(系数很多，通常有上千个，有时甚至多达上 3\n",
    "百万个)都需要计算两次前向传播(计算代价很大)。一种更好的方法是利用网络中所有运算都是`可微(differentiable)`的这一事实，计算损失相对于网络系数的`梯度(gradient)`，然后向梯度的反方向改变系数，从而使损失降低。\n",
    "## 2.导数derivative 和梯度 gradient\n",
    "### 导数\n",
    "导数这个概念我们都不陌生，在微积分里学过，下面我们简单回忆一下导数的概念。\n",
    "\n",
    "假设有一个连续的光滑函数 f(x) = y，将实数 x 映射为另一个实数 y。由于函数是连续的， x 的微小变化只能导致 y 的微小变化——这就是函数连续性的直观解释。假设 x 增大了一个很小的因子epsilon_x，这导致 y 也发生了很小的变化，即 epsilon_y:\n",
    "f(x + epsilon_x) = y + epsilon_y\n",
    "此外，由于函数是光滑的(即函数曲线没有突变的角度)，在某个点 p 附近，如果 epsilon_x 足够小，就可以将 f 近似为斜率为 a 的线性函数，这样epsilon_y 就变成了 a * epsilon_x:\n",
    "f(x + epsilon_x) = y + a * epsilon_x\n",
    "显然，只有在 x 足够接近 p 时，这个线性近似才有效。\n",
    "斜率 a 被称为 f 在 p 点的导数(derivative)。如果 a 是负的，说明 x 在 p 点附近的微小变 7 化将导致 f(x) 减小(如图 2-10 所示);如果 a 是正的，那么 x 的微小变化将导致 f(x) 增大。 此外，a 的绝对值(导数大小)表示增大或减小的速度快慢。\n",
    "![image.png](attachment:image.png)\n",
    "对于每个可微函数 f(x)(可微的意思是“可以被求导”。例如，光滑的连续函数可以被求导)， 都存在一个导数函数 f'(x)，将 x 的值映射为 f 在该点的局部线性近似的斜率。例如，cos(x) 的导数是 -sin(x)，f(x) = a * x 的导数是 f'(x) = a，等等。\n",
    "\n",
    "那么思路就有了，如果你想要将权重w改变一个小因子 epsilon_w，目的是将 f(w) 最小化，并且知道 f 的导数， 那么问题解决了:导数完全描述了改变 w 后 f(w) 会如何变化。如果你希望减小 f(w) 的值，只需将 w 沿着导数的反方向移动一小步（至于一小步是多小，需要根据不同任务自行衡量，也就是损失函数）。\n",
    "也就是使\n",
    "w=w+delta_w\n",
    "delta_W=误差plus（负的导数）plus比率\n",
    "### 梯度\n",
    "一元函数叫导数，多元函数的导数就叫梯度。衡量整个多元函数的变化趋势。\n",
    "`梯度(gradient)`是张量运算的导数。它是导数这一概念向多元函数导数的推广。多元函数是以张量作为输入的函数。\n",
    "假设有一个输入向量 x、一个矩阵 W、一个目标 y 和一个损失函数 loss。你可以用 W 来计 算预测值 y_pred，然后计算损失，或者说预测值 y_pred 和目标 y 之间的距离。\n",
    "```\n",
    "y_pred = dot(W, x) \n",
    "loss_value = loss(y_pred, y)\n",
    "```\n",
    "如果输入数据 x 和 y 保持不变，那么这可以看作将 W 映射到损失值的函数。 loss_value = f(W)\n",
    "假设 W 的当前值为 W0。f 在 W0 点的导数是一个张量 gradient(f)(W0)，其形状与 W 相同， 每个系数 gradient(f)(W0)[i, j] 表示改变 W0[i, j] 时 loss_value 变化的方向和大小。 张量 gradient(f)(W0) 是函数 f(W) = loss_value 在 W0 的导数。\n",
    "前面已经看到，单变量函数 f(x) 的导数可以看作函数 f 曲线的斜率。同样，gradient(f) (W0) 也可以看作表示 f(W) 在 W0 附近`曲率(curvature)的张量。`\n",
    "对于一个函数 f(x)，你可以通过将 x 向导数的反方向移动一小步来减小 f(x) 的值。\n",
    "同样，对于张量的函数 f(W)，你也可以通过将 W 向梯度的反方向移动来减小 f(W)，比如 W1 = W0 - step * gradient(f)(W0)，其中 step 是一个很小的比例因子。也就是说，沿着曲 率的反方向移动，直观上来看在曲线上的位置会更低。\n",
    "注意，比例因子 step 是必需的，因为 gradient(f)(W0) 只是 W0 附近曲率的近似值，不能离 W0 太远。\n",
    "## 3.随机梯度下降\n",
    "给定一个可微函数，理论上可以用解析法找到它的最小值:函数的最小值是导数为 0 的点， 因此你只需找到所有导数为 0 的点，然后计算函数在其中哪个点具有最小值。\n",
    "将这一方法应用于神经网络，就是用解析法求出最小损失函数对应的所有权重值。可以通 过对方程 gradient(f)(W) = 0 求解 W 来实现这一方法。这是包含 N 个变量的多项式方程， 其中 N 是网络中系数的个数。N=2 或 N=3 时可以对这样的方程求解，但对于实际的神经网络是 无法求解的，因为参数的个数不会少于几千个，而且经常有上千万个。\n",
    "相反，你可以使用前面总结的四步算法:基于当前在随机数据批量上的损失，一点一点地对参数进行调节。由于处理的是一个可微函数，你可以计算出它的梯度，从而有效地实 现第四步。沿着梯度的反方向更新权重，损失每次都会变小一点。\n",
    "\n",
    ">(1) 抽取训练样本x和对应目标y组成的数据批量。\n",
    "(2) 在x上运行网络，得到预测值y_pred。\n",
    "(3) 计算网络在这批数据上的损失，用于衡量y_pred和y之间的距离。\n",
    "(4) 计算损失相对于网络参数的梯度[一次反向传播(backward pass)]。\n",
    "(5) 将参数沿着梯度的反方向移动一点，比如 W -= step * gradient，从而使这批数据\n",
    "上的损失减小一点。\n",
    "\n",
    "这很简单!我刚刚描述的方法叫作`小批量随机梯度下降(mini-batch stochastic gradient descent，又称为小批量 SGD)`。术语随机(stochastic)是指每批数据都是随机抽取的(stochastic 是 random 3 在科学上的同义词，下图给出了一维的情况，网络只有一个参数，并且只有一个训练样本\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "如你所见，直观上来看，为 step 因子选取合适的值是很重要的。如果取值太小，则沿着 曲线的下降需要很多次迭代，而且可能会陷入局部极小点。如果取值太大，则更新权重值之后 可能会出现在曲线上完全随机的位置。\n",
    "注意，小批量 SGD 算法的一个变体是每次迭代时只抽取一个样本和目标，而不是抽取一批 数据。这叫作`真 SGD(有别于小批量 SGD)`。还有另一种极端，每一次迭代都在所有数据上运行，这叫作`批量SGD`。这样做的话，每次更新都更加准确，但计算代价也高得多。这两个极端之间的有效折中则是选择合理的批量大小。\n",
    "也就是说，我们从数据集中抽取了一个批次（batchsize）的数据，经过神经网络一个前向传播后，再根据输出结果的误差，从梯度负方向更新前一层参数，依次向前直到更新完整个网络的权重，比如说，前面的mnist数据集，batchsize为128，输入的就是一个二维的张量（128，784），然后从最后向前，对张量求梯度，然后反向传播\n",
    "\n",
    "上图描述的是一维参数空间中的梯度下降，但在实践中需要在高维空间中使用梯度下降。 神经网络的每一个权重参数都是空间中的一个自由维度，网络中可能包含数万个甚至上百万个 参数维度。为了让你对损失曲面有更直观的认识，你还可以将梯度下降沿着二维损失曲面可视化， 如下图所示。但你不可能将神经网络的实际训练过程可视化，因为你无法用人类可以理解的 方式来可视化 1 000 000 维空间。因此最好记住，在这些低维表示中形成的直觉在实践中不一定 总是准确的。这在历史上一直是深度学习研究的问题来源。\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "此外，SGD 还有多种变体，其区别在于计算下一次权重更新时还要考虑上一次权重更新， 而不是仅仅考虑当前梯度值，比如带动量的 SGD、Adagrad、RMSProp 等变体。这些变体被称 为优化方法(optimization method)或优化器(optimizer)。其中动量的概念尤其值得关注，它在 许多变体中都有应用。动量解决了 SGD 的两个问题:收敛速度和局部极小点。下图给出了 损失作为网络参数的函数的曲线。\n",
    "![image.png](attachment:image.png)\n",
    "如你所见，在某个参数值附近，有一个`局部极小点(local minimum)`:在这个点附近，向 左移动和向右移动都会导致损失值增大。如果使用小学习率的 SGD 进行优化，那么优化过程可 能会陷入局部极小点，导致无法找到全局最小点。\n",
    "\n",
    "使用动量方法可以避免这样的问题，这一方法的灵感来源于物理学。有一种有用的思维图像， 就是将优化过程想象成一个小球从损失函数曲线上滚下来。如果小球的动量足够大，那么它不会 卡在峡谷里，最终会到达全局最小点。动量方法的实现过程是每一步都移动小球，不仅要考虑当 前的斜率值(当前的加速度)，还要考虑当前的速度(来自于之前的加速度)。这在实践中的是指， 更新参数 w 不仅要考虑当前的梯度值，还要考虑上一次的参数更新，其简单实现如下所示。\n",
    "```\n",
    "past_velocity = 0.\n",
    "momentum = 0.1 不变的动量因子 \n",
    "while loss > 0.01: 优化循环\n",
    "w, loss, gradient = get_current_parameters()\n",
    "velocity = past_velocity * momentum - learning_rate * gradient w = w + momentum * velocity - learning_rate * gradient past_velocity = velocity\n",
    "update_parameter(w)\n",
    "```\n",
    "# 链式求导:反向传播算法(BP算法）\n",
    "在前面的算法中，我们假设函数是可微的，因此可以明确计算其导数。在实践中，神经网 络函数包含许多连接在一起的张量运算，每个运算都有简单的、已知的导数。例如，下面这个 网络f包含 3 个张量运算a、b和c，还有 3 个权重矩阵W1、W2和W3。\n",
    "f(W1, W2, W3) = a(W1, b(W2, c(W3)))\n",
    "\n",
    "根据微积分的知识，这种函数链可以利用下面这个恒等式进行求导，它称为`链式法则(chain rule)`:(f(g(x)))' = f'(g(x)) * g'(x)。将链式法则应用于神经网络梯度值的计算，得到的算法叫作反向传播(backpropagation，有时也叫反式微分，reverse-mode differentiation)。反向传播从最终损失值开始，从最顶层反向作用至最底层，利用链式法则计算每个参数对损失值的贡献大小。\n",
    "\n",
    "随着深度学习框架的普及，人们将使用能够进行`符号微分(symbolic differentiation)`的现代框架来 实现神经网络，比如 TensorFlow。也就是说，给定一个运算链，并且已知每个运算的导数，这 些框架就可以利用链式法则来计算这个运算链的梯度函数，将网络参数值映射为梯度值。对于这样的函数，反向传播就简化为调用这个梯度函数。由于符号微分的出现，你无须手动实现反向传播算法。\n",
    "\n",
    "如果你希望了解BP反向传播的具体数学推导，可以看我的这篇文章\n",
    "\n",
    "# 回顾\n",
    "已经看完了梯度下降和反向传播，现在应该对神经网络背后的原理有了大致的了解。我们回头 看一下第一个例子，并根据前面三节学到的内容来重新阅读这个例子中的每一段代码。\n",
    "下面是输入数据。\n",
    "```\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "train_images = train_images.reshape((60000, 28 * 28)) train_images = train_images.astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28 * 28)) test_images = test_images.astype('float32') / 255\n",
    "```\n",
    "现在你明白了，输入图像保存在 float32 格式的 Numpy 张量中，形状分别为 (60000, 784)(训练数据)和 (10000, 784)(测试数据)。\n",
    "\n",
    "下面是构建网络。\n",
    "```\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,))) network.add(layers.Dense(10, activation='softmax'))\n",
    "```\n",
    "现在你明白了，这个网络包含两个 Dense(全联接）层，每层都对输入数据进行一些简单的张量运算， 这些运算都包含权重张量。权重张量是该层的属性，里面保存了网络所学到的知识(knowledge)。\n",
    "\n",
    "下面是网络的编译。\n",
    "```\n",
    "network.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "```\n",
    "现在你明白了，categorical_crossentropy 是损失函数，是用于学习权重张量的反馈 信号，在训练阶段应使它最小化。你还知道，减小损失是通过小批量随机梯度下降来实现的。 梯度下降的具体方法由第一个参数给定，即 rmsprop 优化器。\n",
    "\n",
    "最后，下面是训练循环。\n",
    "```\n",
    "network.fit(train_images, train_labels, epochs=5, batch_size=128)\n",
    "```\n",
    "现在你明白在调用 fit 时发生了什么:网络开始在训练数据上进行迭代(每个小批量包含 128 个样本)，共迭代 5 次[在所有训练数据上迭代一次叫作一个轮次(epoch)]。在每次迭代 过程中，网络会计算批量损失相对于权重的梯度，并相应地更新权重。5 轮之后，网络进行了 2345 次梯度更新(每轮 469 次)，网络损失值将变得足够小，使得网络能够以很高的精度对手 写数字进行分类。\n",
    "到目前为止，你已经了解了神经网络的大部分知识。\n",
    "\n",
    "最后我们明白了，神经网络通过存储权重的方式学习的知识，我们通过反向传播方法（一般采取梯度下降方法）来训练其中的权重，训练完成后，我们就不需要再使用反向传播来，直接将任务输入神经网络，就可以得到结果了。\n",
    "\n",
    "这个过程就类似我们上学考试，平常我们做试卷，然后对答案，知道自己的结果和答案的偏差以后纠正自己的知识。最后考试的时候我们就只做试卷就可以了。\n",
    "\n",
    "# 参考文献\n",
    "> [1]《机器学习》周志华著，清华大学出版社,2016. \n",
    "[2]  Python深度学习，（美）弗朗索瓦·肖莱，人民邮电出版社，2018，8\n",
    "[3] 深度学习，[美] 伊恩·古德费洛 / [加] 约书亚·本吉奥 / [加] 亚伦·库维尔 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
